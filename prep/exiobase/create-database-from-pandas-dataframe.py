# -*- coding: utf-8 -*-
"""Create Database from Pandas Dataframe

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IMpOYzT6oXbZXaJKugi5vCmUB_tIHo0J

Project overview:  
https://model.earth/OpenFootprint/trade
"""

import pandas as pd

def infer_data_types(df, db_type='postgres'):
    """
    Infers SQL data types for each column in a DataFrame based on its contents,
    and adjusts the inferred type based on the database type (PostgreSQL or DuckDB).

    Args:
    df (pd.DataFrame): The DataFrame to infer data types for.
    db_type (str): The type of database to infer types for ('postgres' or 'duckdb').

    Returns:
    dict: A dictionary with DataFrame column names as keys and inferred SQL data types as values.
    """

    inferred_types = {}

    for column_name, column_data in df.items():
        if pd.api.types.is_integer_dtype(column_data):
            if db_type == 'postgres':
                inferred_types[column_name] = 'INT'
            elif db_type == 'duckdb':
                inferred_types[column_name] = 'BIGINT'  # DuckDB uses BIGINT for integer types
        elif pd.api.types.is_float_dtype(column_data):
            if db_type == 'postgres':
                inferred_types[column_name] = 'FLOAT'
            elif db_type == 'duckdb':
                inferred_types[column_name] = 'DOUBLE'  # DuckDB uses DOUBLE for floating point numbers
        elif pd.api.types.is_bool_dtype(column_data):
            inferred_types[column_name] = 'BOOLEAN'  # BOOLEAN is supported in both PostgreSQL and DuckDB
        elif pd.api.types.is_datetime64_any_dtype(column_data):
            inferred_types[column_name] = 'TIMESTAMP'  # TIMESTAMP is supported in both PostgreSQL and DuckDB
        else:
            # Handle string types with VARCHAR based on max length
            max_length = column_data.astype(str).map(len).max()
            if db_type == 'postgres':
                inferred_types[column_name] = f'VARCHAR({2 * max_length})' if max_length > 0 else 'VARCHAR(255)'
            elif db_type == 'duckdb':
                inferred_types[column_name] = f'VARCHAR({2 * max_length})' if max_length > 0 else 'VARCHAR'

    return inferred_types


def generate_create_table_sql(table_name, df, db_type='postgres'):
    """
    Generates a SQL CREATE TABLE statement from the DataFrame,
    automatically inferring data types from the DataFrame.

    Args:
    table_name (str): Name of the table to be created.
    df (pd.DataFrame): The DataFrame from which to infer data types and column names.
    db_type (str): The type of database ('postgres' or 'duckdb').

    Returns:
    str: A SQL CREATE TABLE statement.
    """

    # Infer data types for the DataFrame
    inferred_types = infer_data_types(df, db_type=db_type)

    # Start the CREATE TABLE statement
    create_stmt = f"CREATE TABLE {table_name} ("

    # Generate column definitions using DataFrame column names
    column_definitions = [f"{col} {inferred_types[col]}" for col in df.columns]

    # Join column definitions and complete the SQL statement
    create_stmt += ", ".join(column_definitions) + ");"
    return create_stmt


def generate_insert_sql(table_name, df, db_type='postgres'):
    """
    Generates a SQL INSERT statement for a DataFrame, adjusting for PostgreSQL or DuckDB placeholder styles.

    Args:
    table_name (str): Name of the SQL table.
    df (pd.DataFrame): The DataFrame containing the data to be inserted.
    db_type (str): The type of database ('postgres' or 'duckdb').

    Returns:
    tuple: A SQL INSERT INTO statement and a list of lists of values for parameterized execution.
    """
    # Determine placeholder based on database type
    placeholder = '%s' if db_type == 'postgres' else '?'

    # Use the DataFrame's column names for the INSERT statement
    column_names = ", ".join(df.columns)
    placeholders = ", ".join([placeholder for _ in df.columns])
    insert_stmt = f"INSERT INTO {table_name} ({column_names}) VALUES ({placeholders});"

    # Convert NaN to None for SQL NULL compatibility and prepare values for all rows
    values = [
        [value if pd.notnull(value) else None for value in row]
        for _, row in df.iterrows()
    ]

    return insert_stmt, values
def create_and_insert_tables_final(cursor, table_name, df, db_type='postgres'):
    """
    Creates a SQL table and inserts data from a DataFrame into it using bulk insertion.

    Args:
    cursor: The cursor to execute SQL commands.
    table_name (str): The name of the SQL table.
    df (pd.DataFrame): The DataFrame containing the data to be inserted.
    db_type (str): The type of database ('postgres' or 'duckdb').
    """

    # Step 1: Generate and execute the CREATE TABLE statement
    try:
        create_table_sql = generate_create_table_sql(table_name, df, db_type=db_type)
        cursor.execute(create_table_sql)
        print(f"Table '{table_name}' created successfully.")
    except Exception as e:
        print(f"Error creating table '{table_name}': {e}")
        return

    # Step 2: Generate the INSERT statement and values
    try:
        insert_stmt, insert_values = generate_insert_sql(table_name, df, db_type=db_type)

        # Step 3: Execute the INSERT statement using executemany for bulk insertion
        cursor.executemany(insert_stmt, insert_values)
        print(f"Data inserted successfully into '{table_name}'.")
    except Exception as e:
        print(f"Error inserting data into '{table_name}': {e}")

